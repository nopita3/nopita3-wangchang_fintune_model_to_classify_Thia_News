{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer ,AutoModelForCausalLM\n",
    "model_TH = \"airesearch/wangchanberta-base-att-spm-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fb2033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 23:18:55.379909: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-28 23:18:55.682493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 23:18:57.409941: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer \n",
    "model = AutoModel.from_pretrained(model_TH, \n",
    "                                 device_map=\"cuda\", \n",
    "                                 torch_dtype=\"auto\", \n",
    "                                 trust_remote_code=True,\n",
    "                                 ) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_TH, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a69e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_th = \"\"\"\n",
    "‡πÉ‡∏ô‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ ‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÇ‡∏î‡∏¢‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏´‡∏ô‡∏∏‡∏ô‡∏à‡∏≤‡∏Å‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ö‡∏ß‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£ ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ß‡πà‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏•‡∏≤‡∏á‡∏™‡∏´‡∏£‡∏±‡∏ê‡∏Ø ‡∏≠‡∏≤‡∏à‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\n",
    "‡∏ô‡∏±‡∏Å‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πà‡∏≠‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î ‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÅ‡∏£‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô‡∏´‡∏∏‡πâ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏ó‡∏≤‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏£‡∏±‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡πÑ‡∏î‡πâ\n",
    "‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\n",
    "\"\"\"\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer(text_th, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f57562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,    10, 11228,   247,    10,  7953,   109,  2731,   107,    58,\n",
       "           652,    10, 16778,   611,  3291,    32,  2016,  1867, 13452,    10,\n",
       "            98, 15279,    16,   283,   306,   136,   703, 15102,     9, 16034,\n",
       "          1236,  8801, 11997,  1348,  2599,  1590,    10,  4262,   372,  2119,\n",
       "           429,   100,  2928,    16,   582, 13760, 15781,   238,  5440,  1563,\n",
       "           171,   208,  3553,   171,  1174,  6269,  1045,  1589,    90,  4443,\n",
       "          2400, 14445,   946,    83,   649,   497,    63, 13512,  2348,  6198,\n",
       "            12, 20330,  6390, 21619,    16,   582,    15,  1609,    10,  2622,\n",
       "           946,  2381,  1297,  2581,   283,   679,  2203,   379,  7031,    13,\n",
       "          1864,    10, 17073,  1297, 14997, 21264,   954,  3743,   410,   283,\n",
       "           109,  7560,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2153f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = {tokenizer.decode(word):word for word in tokens['input_ids'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb13587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_TH)\n",
    "output = model(**tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88cef7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5875, -1.4182,  0.1009,  ...,  0.1197,  0.2237,  0.1228],\n",
       "         [-0.3282, -1.8049,  0.1698,  ...,  0.2133,  1.7585,  0.4397],\n",
       "         [-1.7790, -1.3772,  1.2845,  ..., -0.2261, -1.9724,  0.3860],\n",
       "         ...,\n",
       "         [-0.9275, -1.3331, -0.9744,  ...,  1.4161,  0.6069, -2.2569],\n",
       "         [-0.7577, -0.0811,  0.6786,  ...,  0.8232,  0.1763, -1.2139],\n",
       "         [-0.5875, -1.4182,  0.1009,  ...,  0.1197,  0.2237,  0.1228]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e55bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "fill_mask = pipeline(task='fill-mask',\n",
    "         tokenizer=tokenizer,\n",
    "         model = model_TH,\n",
    "         revision = 'main',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a315f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask_pad(input_text):\n",
    "    return fill_mask(input_text+\"<mask>\"+'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac59c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏ô‡∏±‡∏Å‡πÄ‡∏•‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏£‡∏±‡∏ö\n",
      "‡∏ô‡∏±‡∏Å‡πÄ‡∏•‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏£‡∏±‡∏ö<unk>\n",
      "‡∏ô‡∏±‡∏Å‡πÄ‡∏•‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏£‡∏±‡∏ö<unk>üòÇ\n",
      "‡∏ô‡∏±‡∏Å‡πÄ‡∏•‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏£‡∏±‡∏ö<unk>üòÇü§£\n",
      "‡∏ô‡∏±‡∏Å‡πÄ‡∏•‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏£‡∏±‡∏ö<unk>üòÇü§£ü§£\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_text = \"‡∏ô‡∏±‡∏Å‡πÄ‡∏•‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤\" \n",
    "for i in range(5):\n",
    "    word_list= fill_mask_pad(input_text)\n",
    "    c = np.random.randint(0,5,3)\n",
    "    input_text += word_list[c[0]]['token_str']\n",
    "    print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f254b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Using a valid sentiment analysis model for Thai text\n",
    "classify_multiclass = pipeline(task='sentiment-analysis',\n",
    "                               tokenizer=tokenizer,\n",
    "                               model='airesearch/wangchanberta-base-att-spm-uncased',\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0309c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_train = pd.read_csv('prachatai_train.csv')\n",
    "data_test = pd.read_csv('prachatai_test.csv')\n",
    "data_val = pd.read_csv('prachatai_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9d2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    'politics',\n",
    "    'human_rights',\n",
    "    'quality_of_life',\n",
    "    'international',\n",
    "    'social',\n",
    "    'environment',\n",
    "    'economics',\n",
    "    'culture',\n",
    "    'labor',\n",
    "    'national_security',\n",
    "    'ict',\n",
    "    'education',\n",
    "]\n",
    "\n",
    "dict_tag_train = {tag:int(data_train[tag].value_counts()[1]) for tag in tags} \n",
    "dict_tag_test = {tag:int(data_test[tag].value_counts()[1]) for tag in tags} \n",
    "dict_tag_val = {tag:int(data_val[tag].value_counts()[1]) for tag in tags} \n",
    "\n",
    "df_count_train = pd.DataFrame(list(dict_tag_train.items()), columns=['tag', 'count'])\n",
    "df_count_test = pd.DataFrame(list(dict_tag_test.items()), columns=['tag', 'count'])\n",
    "df_count_val = pd.DataFrame(list(dict_tag_val.items()), columns=['tag', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02355ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>3852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human_rights</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quality_of_life</td>\n",
       "      <td>1144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>international</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>environment</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>economics</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>culture</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>labor</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>national_security</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ict</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>education</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tag  count\n",
       "0            politics   3852\n",
       "1        human_rights   1458\n",
       "2     quality_of_life   1144\n",
       "3       international    828\n",
       "4              social    782\n",
       "5         environment    764\n",
       "6           economics    487\n",
       "7             culture    388\n",
       "8               labor    375\n",
       "9   national_security    339\n",
       "10                ict    285\n",
       "11          education    248"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa79cb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc252dc3c9242bb8ee978864953cb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da55b5ab89944aca4b99afe860fe109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification , TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_TH,\n",
    "    num_labels=len(tags),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    tokens = tokenizer(example['body_text'], \n",
    "                      truncation=True,\n",
    "                      max_length=MAX_LEN,\n",
    "                      padding=False)  # Don't pad here, let DataCollator handle it\n",
    "    # Add labels for multi-label classification\n",
    "    labels = [float(example[tag]) for tag in tags]\n",
    "    tokens['labels'] = labels\n",
    "    return tokens\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(data_train)\n",
    "val_dataset = Dataset.from_pandas(data_val)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized = {\n",
    "    \"train\": train_dataset.map(tokenize_fn, remove_columns=train_dataset.column_names),\n",
    "    \"validation\": val_dataset.map(tokenize_fn, remove_columns=val_dataset.column_names)\n",
    "}\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68233298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5,\n",
       "  10,\n",
       "  601,\n",
       "  1003,\n",
       "  425,\n",
       "  11236,\n",
       "  6840,\n",
       "  641,\n",
       "  9550,\n",
       "  2173,\n",
       "  7130,\n",
       "  8565,\n",
       "  10,\n",
       "  1388,\n",
       "  14155,\n",
       "  166,\n",
       "  26,\n",
       "  10575,\n",
       "  336,\n",
       "  5129,\n",
       "  7642,\n",
       "  7801,\n",
       "  83,\n",
       "  1370,\n",
       "  947,\n",
       "  32,\n",
       "  605,\n",
       "  480,\n",
       "  357,\n",
       "  10,\n",
       "  76,\n",
       "  162,\n",
       "  11,\n",
       "  10,\n",
       "  2425,\n",
       "  5210,\n",
       "  3272,\n",
       "  10496,\n",
       "  275,\n",
       "  2982,\n",
       "  425,\n",
       "  5221,\n",
       "  734,\n",
       "  472,\n",
       "  947,\n",
       "  636,\n",
       "  1984,\n",
       "  641,\n",
       "  985,\n",
       "  497,\n",
       "  3548,\n",
       "  2633,\n",
       "  17663,\n",
       "  16,\n",
       "  21727,\n",
       "  10,\n",
       "  12,\n",
       "  622,\n",
       "  1304,\n",
       "  12430,\n",
       "  35,\n",
       "  600,\n",
       "  501,\n",
       "  112,\n",
       "  513,\n",
       "  8565,\n",
       "  952,\n",
       "  2643,\n",
       "  746,\n",
       "  10,\n",
       "  8519,\n",
       "  7515,\n",
       "  2190,\n",
       "  10,\n",
       "  579,\n",
       "  9130,\n",
       "  2124,\n",
       "  4526,\n",
       "  7182,\n",
       "  351,\n",
       "  8565,\n",
       "  3925,\n",
       "  22,\n",
       "  10,\n",
       "  409,\n",
       "  10,\n",
       "  1123,\n",
       "  10,\n",
       "  3478,\n",
       "  97,\n",
       "  668,\n",
       "  2246,\n",
       "  10,\n",
       "  460,\n",
       "  377,\n",
       "  10,\n",
       "  65,\n",
       "  10,\n",
       "  168,\n",
       "  222,\n",
       "  622,\n",
       "  702,\n",
       "  980,\n",
       "  567,\n",
       "  968,\n",
       "  760,\n",
       "  8565,\n",
       "  308,\n",
       "  45,\n",
       "  10,\n",
       "  4435,\n",
       "  8953,\n",
       "  8565,\n",
       "  2499,\n",
       "  1898,\n",
       "  4096,\n",
       "  10,\n",
       "  5098,\n",
       "  7443,\n",
       "  2212,\n",
       "  46,\n",
       "  289,\n",
       "  10,\n",
       "  22560,\n",
       "  32,\n",
       "  760,\n",
       "  640,\n",
       "  6],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"train\"].take(2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d11993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert to numpy arrays if they aren't already\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = 1 / (1 + np.exp(-logits))  # sigmoid function\n",
    "    # Convert to binary predictions\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    y_true = labels.astype(int)\n",
    "\n",
    "    return {\n",
    "        \"f1_micro\": f1_score(y_true, preds, average=\"micro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(y_true, preds, average=\"macro\", zero_division=0),\n",
    "        \"precision_micro\": precision_score(y_true, preds, average=\"micro\", zero_division=0),\n",
    "        \"recall_micro\": recall_score(y_true, preds, average=\"micro\", zero_division=0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7fa142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8379/3818179347.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = \"wcberta-prachathai67k\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy =\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    remove_unused_columns=True\n",
    "   )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4ffd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 6, 'bos_token_id': 5}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40785' max='40785' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40785/40785 6:47:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>0.171885</td>\n",
       "      <td>0.728551</td>\n",
       "      <td>0.648837</td>\n",
       "      <td>0.782390</td>\n",
       "      <td>0.681644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.165269</td>\n",
       "      <td>0.744290</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.799204</td>\n",
       "      <td>0.696438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.165133</td>\n",
       "      <td>0.756356</td>\n",
       "      <td>0.687707</td>\n",
       "      <td>0.793363</td>\n",
       "      <td>0.722648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40785, training_loss=0.16829485802640515, metrics={'train_runtime': 24469.2873, 'train_samples_per_second': 6.667, 'train_steps_per_second': 1.667, 'total_flos': 1.073175053363712e+16, 'train_loss': 0.16829485802640515, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca52800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e12991bf30b4d63981ead0be25ad95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Dataset.from_pandas(data_test)\n",
    "tokenized_test = test.map(tokenize_fn, remove_columns=test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "718ed0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5,\n",
       "  2117,\n",
       "  10,\n",
       "  40,\n",
       "  11,\n",
       "  40,\n",
       "  11,\n",
       "  10,\n",
       "  569,\n",
       "  312,\n",
       "  972,\n",
       "  271,\n",
       "  54,\n",
       "  231,\n",
       "  2763,\n",
       "  792,\n",
       "  1171,\n",
       "  21,\n",
       "  20573,\n",
       "  24,\n",
       "  12784,\n",
       "  10,\n",
       "  676,\n",
       "  16,\n",
       "  9739,\n",
       "  10934,\n",
       "  1332,\n",
       "  554,\n",
       "  1242,\n",
       "  19,\n",
       "  20549,\n",
       "  1357,\n",
       "  1579,\n",
       "  10,\n",
       "  3,\n",
       "  6974,\n",
       "  18,\n",
       "  222,\n",
       "  19,\n",
       "  10622,\n",
       "  327,\n",
       "  17624,\n",
       "  8159,\n",
       "  18,\n",
       "  10,\n",
       "  6762,\n",
       "  1802,\n",
       "  7862,\n",
       "  2328,\n",
       "  14137,\n",
       "  14262,\n",
       "  108,\n",
       "  1065,\n",
       "  622,\n",
       "  10,\n",
       "  17318,\n",
       "  9,\n",
       "  9563,\n",
       "  10,\n",
       "  396,\n",
       "  56,\n",
       "  9,\n",
       "  10300,\n",
       "  10,\n",
       "  3835,\n",
       "  979,\n",
       "  50,\n",
       "  755,\n",
       "  350,\n",
       "  2970,\n",
       "  13,\n",
       "  676,\n",
       "  4033,\n",
       "  10,\n",
       "  16,\n",
       "  9739,\n",
       "  10934,\n",
       "  1332,\n",
       "  599,\n",
       "  273,\n",
       "  2216,\n",
       "  32,\n",
       "  2843,\n",
       "  79,\n",
       "  19,\n",
       "  3,\n",
       "  3234,\n",
       "  21433,\n",
       "  1579,\n",
       "  10,\n",
       "  3,\n",
       "  6974,\n",
       "  18,\n",
       "  10,\n",
       "  9388,\n",
       "  1351,\n",
       "  9401,\n",
       "  12,\n",
       "  215,\n",
       "  144,\n",
       "  569,\n",
       "  584,\n",
       "  713,\n",
       "  231,\n",
       "  271,\n",
       "  2407,\n",
       "  379,\n",
       "  1071,\n",
       "  10,\n",
       "  3748,\n",
       "  14137,\n",
       "  14262,\n",
       "  108,\n",
       "  31,\n",
       "  287,\n",
       "  1522,\n",
       "  673,\n",
       "  2117,\n",
       "  10,\n",
       "  40,\n",
       "  11,\n",
       "  40,\n",
       "  11,\n",
       "  10,\n",
       "  559,\n",
       "  10,\n",
       "  3082,\n",
       "  6],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test.take(2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c13a3ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205c1b64efcb49d1a6cc28b9ab07da8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = data_test.loc[1:2,['body_text']+tags]\n",
    "test_data = Dataset.from_pandas(test_data)\n",
    "test_data_tok = test_data.map(tokenize_fn, remove_columns=test_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "251f4cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body_text': '‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û ‡∏ô‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô ‡∏†‡∏≤‡∏Ñ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏° 15 ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡πà‡∏ß‡∏°‡πÄ‡∏î‡∏¥‡∏ô‡∏£‡∏ì‡∏£‡∏á‡∏Ñ‡πå‡∏¢‡∏∏‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ\\n\\n\\n\\n‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 8 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° 2559 ‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏ä‡∏≤‡∏¢‡πÅ‡∏î‡∏ô‡πÉ‡∏ï‡πâ‡πÅ‡∏•‡∏∞‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏†‡∏≤‡∏Ñ‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° ‚Äú‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏´‡∏¢‡∏∏‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏î‡πá‡∏Å‚Äù ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏õ‡∏µ 2559 ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏£‡∏ì‡∏£‡∏á‡∏Ñ‡πå‡∏¢‡∏∏‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ ‡∏ú‡πà‡∏≤‡∏ô‡∏ñ‡∏ô‡∏ô‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏´‡∏≠‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏≠‡∏ò‡∏¥‡∏Å‡∏≤‡∏£‡∏ö‡∏î‡∏µ ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏á‡∏Ç‡∏•‡∏≤‡∏ô‡∏Ñ‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå (‡∏°.‡∏≠.) ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏Ç‡∏ï‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ñ‡∏•‡∏á‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 500 ‡∏Ñ‡∏ô\\n‡πÇ‡∏î‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô‡πÉ‡∏ô‡πÄ‡∏Ç‡∏ï‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏†‡∏≤‡∏Ñ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°15 ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢1.‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏ä‡∏≤‡∏¢‡πÅ‡∏î‡∏ô‡πÉ‡∏ï‡πâ2.‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ3.‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏£‡∏±‡∏ê‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÉ‡∏ï‡πâ‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏°.‡∏≠.‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ4.‡∏ä‡∏°‡∏£‡∏°‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏£‡∏±‡∏Å‡∏™‡∏±‡∏ô‡∏ï‡∏¥5.‡∏ä‡∏°‡∏£‡∏° ‡∏°.‡∏≠.‡∏ó‡∏≥‡∏î‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°6.‡∏ä‡∏∏‡∏°‡∏ô‡∏∏‡∏°‡∏™‡∏¥‡∏á‡∏´‡πå‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û 7.‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏ö‡∏ç‡∏à‡∏°‡∏£‡∏≤‡∏ä‡∏π‡∏ó‡∏¥‡∏®‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ\\n8.‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏î‡∏ä‡∏∞‡∏õ‡∏±‡∏ï‡∏ï‡∏ô‡∏¢‡∏≤‡∏ô‡∏∏‡∏Å‡∏π‡∏•9.‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ10.‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏≠‡∏≤‡∏ä‡∏µ‡∏ß‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ11.‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ12.‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ13.‡∏™‡∏´‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏ä‡∏≤‡∏¢‡πÅ‡∏î‡∏ô‡πÉ‡∏ï‡πâ14.‡∏™‡πÇ‡∏°‡∏™‡∏£‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°.‡∏≠.‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ15.‡∏†‡∏≤‡∏Ñ‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏±‡∏á‡∏Ñ‡∏° ‡∏Ñ‡∏ì‡∏∞‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°.‡∏≠.‡∏õ‡∏±‡∏ï‡∏ï‡∏≤‡∏ô‡∏µ\\n‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏£‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏¢‡∏∏‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏Å‡πà‡πÄ‡∏î‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏¢‡∏≤‡∏ß‡∏ä‡∏ô ‡∏ï‡∏•‡∏≠‡∏î‡∏à‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏™‡∏∏‡∏ó‡∏ò‡∏¥‡πå‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ä‡∏≤‡∏¢‡πÅ‡∏î‡∏ô‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡πÄ‡∏Ç‡∏ï‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ä‡∏≤‡∏¢‡πÅ‡∏î‡∏ô‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡πá‡∏á‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏™‡∏∏‡∏Ç',\n",
       " 'politics': 1,\n",
       " 'human_rights': 0,\n",
       " 'quality_of_life': 0,\n",
       " 'international': 0,\n",
       " 'social': 1,\n",
       " 'environment': 0,\n",
       " 'economics': 0,\n",
       " 'culture': 0,\n",
       " 'labor': 0,\n",
       " 'national_security': 1,\n",
       " 'ict': 0,\n",
       " 'education': 0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.take(2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6717756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['politics', 'social', 'national_security']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_result = trainer.predict(test_data_tok.select(range(2)))\n",
    "topic = [j for i,j in enumerate(tags) if pre_result.label_ids[1][i] == 1]\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e12b3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_for_best_model: None\n",
      "best_metric: 0.7442904548116338\n",
      "best_model_checkpoint: wcberta-prachathai67k/checkpoint-27190\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "run_dir = \"wcberta-prachathai67k/checkpoint-27190\"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ trainer_state.json\n",
    "with open(os.path.join(run_dir, \"trainer_state.json\"), encoding=\"utf-8\") as f:\n",
    "    st = json.load(f)\n",
    "\n",
    "print(\"metric_for_best_model:\", st.get(\"metric_for_best_model\"))\n",
    "print(\"best_metric:\", st.get(\"best_metric\"))\n",
    "print(\"best_model_checkpoint:\", st.get(\"best_model_checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b650ed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_for_best_model: None\n",
      "best_metric: 0.7563563372204167\n",
      "best_model_checkpoint: wcberta-prachathai67k/checkpoint-40785\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "run_dir = \"wcberta-prachathai67k/checkpoint-40785\"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ trainer_state.json\n",
    "with open(os.path.join(run_dir, \"trainer_state.json\"), encoding=\"utf-8\") as f:\n",
    "    st = json.load(f)\n",
    "\n",
    "print(\"metric_for_best_model:\", st.get(\"metric_for_best_model\"))\n",
    "print(\"best_metric:\", st.get(\"best_metric\"))\n",
    "print(\"best_model_checkpoint:\", st.get(\"best_model_checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfebe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "best_ckpt = \"wcberta-prachathai67k/checkpoint-40785\"\n",
    "export_dir = \"wcberta-prachathai67k-best\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_ckpt)\n",
    "tok   = AutoTokenizer.from_pretrained(best_ckpt)\n",
    "\n",
    "model.save_pretrained(export_dir)\n",
    "tok.save_pretrained(export_dir)\n",
    "\n",
    "# (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™)\n",
    "import json\n",
    "LABEL_NAMES = [\"politics\",\"human_rights\",\"quality_of_life\",\"international\",\"social\",\n",
    "               \"environment\",\"economics\",\"culture\",\"labor\",\"national_security\",\"ict\",\"education\"]\n",
    "with open(f\"{export_dir}/label_names.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(LABEL_NAMES, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
