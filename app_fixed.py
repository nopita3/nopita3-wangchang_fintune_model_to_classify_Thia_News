import json, torch, numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit as st
import pandas as pd

st.set_page_config(
    page_title="Thai News Classification",
    page_icon="üì∞",
    layout="centered"
)

MODEL_DIR = "wcberta-prachathai67k-best"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà save_pretrained ‡πÑ‡∏ß‡πâ
MAX_LEN   = 512
THRESH    = 0.5   

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î device ‡∏Å‡πà‡∏≠‡∏ô
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ‡πÇ‡∏´‡∏•‡∏î tokenizer
tok = AutoTokenizer.from_pretrained(MODEL_DIR)

# ‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ meta tensor
try:
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÇ‡∏´‡∏•‡∏î model ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á device ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_DIR, 
        torch_dtype=torch.float32,
        device_map=None,  # ‡∏õ‡∏¥‡∏î automatic device mapping
        low_cpu_mem_usage=False
    )
    model = model.to(device)
    model.eval()
    print("Model loaded successfully with direct device mapping")
    
except Exception as e:
    print(f"Direct loading failed: {e}")
    try:
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ to_empty() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö meta tensor
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
        if hasattr(model, 'to_empty'):
            model = model.to_empty(device=device)
        else:
            model = model.to(device)
        model.eval()
        print("Model loaded successfully with to_empty method")
        
    except Exception as e2:
        print(f"to_empty method failed: {e2}")
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö CPU ‡πÅ‡∏•‡πâ‡∏ß move ‡∏ó‡∏µ‡∏•‡∏∞‡∏ä‡∏¥‡πâ‡∏ô
        model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        model = model.to(device)
        model.eval()
        print("Model loaded successfully with CPU first method")


with open(f"{MODEL_DIR}/label_names.json", encoding="utf-8") as f:
    LABELS = json.load(f)


def predict_with_probs(texts: list[str], threshold: float = THRESH, top_k: int = None):
    """
    ‡∏Ñ‡∏∑‡∏ô:
      - probs_sorted: ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ + prob ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
      - chosen: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå" (threshold ‡∏´‡∏£‡∏∑‡∏≠ top_k)
    """
    try:
        # Tokenize with safe parameters
        enc = tok(texts, return_tensors="pt", 
                  truncation=True, 
                  max_length=MAX_LEN, 
                  padding=True,
                  return_token_type_ids=False)
        enc = {k: v.to(device) for k, v in enc.items()}

        sigmoid = torch.nn.Sigmoid()
        with torch.no_grad():
            logits = model(**enc).logits
            probs = sigmoid(logits).cpu().numpy()  # (B, C)
            
    except Exception as e:
        print(f"Error in tokenization or model prediction: {e}")
        # Return empty results if error occurs
        return [{"text": text, "probs_sorted": [], "chosen": []} for text in texts]

    results = []
    for i, text in enumerate(texts):
        p = probs[i]                                 # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™
        order = np.argsort(p)[::-1]                  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡∏Å‚Üí‡∏ô‡πâ‡∏≠‡∏¢
        probs_sorted = [(LABELS[j], float(p[j])) for j in order]

        if top_k and top_k > 0:
            chosen = [LABELS[j] for j in order[:top_k]]
        else:
            chosen = []
            for name, prob in probs_sorted:
                if prob >= threshold:
                    chosen.append(name)
                else:
                    break

        results.append({
            "text": text,
            "probs_sorted": probs_sorted,  # ‡∏î‡∏π‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ
            "chosen": chosen               # ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™
        })
    return results

st.title("üì∞ Thai News Classification")
st.markdown("‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• `wcberta-prachathai67k`")

st.write("‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)")
input_texts = st.text_area("‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 512 ‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î):", height=200, placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏Å‡∏£‡∏°‡∏≠‡∏∏‡∏ï‡∏∏‡∏Ø ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏ó‡∏¢‡πÄ‡∏à‡∏≠‡∏ù‡∏ô‡∏ñ‡∏•‡πà‡∏°...")

if st.button("üß† ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•"):
    if not input_texts.strip():
        st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•")
    else:
        valid_texts = [input_texts.strip()]
        for text in valid_texts:
            if len(text.split()) > 512:
                st.error(f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà  ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 512 ‡∏Ñ‡∏≥: '{text[:50]}...'")
            

        if valid_texts:
            with st.spinner("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°..."):
                try:
                    results = predict_with_probs(valid_texts, threshold=None, top_k=3)
                    st.markdown(f"‡∏ú‡∏• {results}")
                    
                    if results :
                        st.success("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                        
                        st.subheader("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
                        for result in results:
                            with st.container(border=True):
                                st.markdown(f"**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:** {result['text']}")
                                
                                tags = ' '.join([f"`{cat}`" for cat in result['chosen']])
                                st.markdown(f"**‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:** {tags}")

                                with st.expander("‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
                                    df = pd.DataFrame(result['probs_sorted'], columns=['‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô'])
                                    st.dataframe(df, width='stretch')
                    else:
                        st.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
                        
                except Exception as e:
                    st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
                    st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")