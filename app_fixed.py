import json, torch, numpy as np
import traceback
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit as st
import pandas as pd

st.set_page_config(
    page_title="Thai News Classification",
    page_icon="üì∞",
    layout="centered"
)

MODEL_DIR = "wcberta-prachathai67k-best"
MAX_LEN = 512
THRESH = 0.5   

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î device ‡∏Å‡πà‡∏≠‡∏ô
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize session state for model
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.session_state.error_message = ""

def load_model_and_tokenizer():
    """‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏•‡∏∞ tokenizer ‡∏û‡∏£‡πâ‡∏≠‡∏° error handling"""
    try:
        print(f"Loading tokenizer from {MODEL_DIR}...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
        
        print(f"Loading model from {MODEL_DIR}...")
        # ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î model
        model = None
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        try:
            model = AutoModelForSequenceClassification.from_pretrained(
                MODEL_DIR, 
                dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=False
            )
            model = model.to(device)
            model.eval()
            print("‚úÖ Model loaded successfully with method 1")
            
        except Exception as e1:
            print(f"‚ùå Method 1 failed: {e1}")
            
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ to_empty()
            try:
                model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
                if hasattr(model, 'to_empty'):
                    model = model.to_empty(device=device)
                else:
                    model = model.to(device)
                model.eval()
                print("‚úÖ Model loaded successfully with method 2")
                
            except Exception as e2:
                print(f"‚ùå Method 2 failed: {e2}")
                
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô CPU ‡∏Å‡πà‡∏≠‡∏ô
                try:
                    model = AutoModelForSequenceClassification.from_pretrained(
                        MODEL_DIR,
                        dtype=torch.float32
                    )
                    model = model.to(device)
                    model.eval()
                    print("‚úÖ Model loaded successfully with method 3")
                except Exception as e3:
                    print(f"‚ùå All methods failed: {e3}")
                    raise e3
        
        # ‡πÇ‡∏´‡∏•‡∏î labels
        with open(f"{MODEL_DIR}/label_names.json", encoding="utf-8") as f:
            labels = json.load(f)
        
        print(f" ‚úÖ All components loaded successfully! ")
        print(f"üìä Device: {device}")
        print(f"üìö Vocab size: {tokenizer.vocab_size}")
        print(f"üè∑Ô∏è Number of labels: {len(labels)}")
        
        return model, tokenizer, labels, ""
        
    except Exception as e:
        error_msg = f"Failed to load model: {str(e)}\n{traceback.format_exc()}"
        print(f"‚ùå {error_msg}")
        return None, None, None, error_msg

def predict_with_probs(texts: list[str], model, tokenizer, labels, threshold: float = THRESH, top_k: int = None):
    """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° detailed error reporting"""
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ model ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß
    if model is None or tokenizer is None:
        error_msg = "Model ‡∏´‡∏£‡∏∑‡∏≠ tokenizer ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î"
        print(f"‚ùå {error_msg}")
        return [{"text": text, "probs_sorted": [], "chosen": [], "error": error_msg} for text in texts]
    
    try:
        print(f"üî§ Input texts: {texts}")
        print(f"üìè Text lengths: {[len(text) for text in texts]}")
        
        # Tokenize
        enc = tokenizer(
            texts, 
            return_tensors="pt", 
            truncation=True, 
            max_length=MAX_LEN, 
            padding=True,
            add_special_tokens=True,
            return_attention_mask=True,
            return_token_type_ids=False
        )
        
        print(f"üî¢ Token shape: {enc['input_ids'].shape}")
        print(f"üìä Token range: {enc['input_ids'].min()} - {enc['input_ids'].max()}")
        print(f"üìö Vocab size: {tokenizer.vocab_size}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö token IDs
        vocab_size = tokenizer.vocab_size
        if torch.any(enc['input_ids'] >= vocab_size):
            print(f"‚ö†Ô∏è WARNING: Found token IDs >= vocab_size ({vocab_size})")
            enc['input_ids'] = torch.clamp(enc['input_ids'], 0, vocab_size - 1)
            
        if torch.any(enc['input_ids'] < 0):
            print("‚ö†Ô∏è WARNING: Found negative token IDs")
            enc['input_ids'] = torch.clamp(enc['input_ids'], 0, vocab_size - 1)
        
        # Move to device
        enc = {k: v.to(device) for k, v in enc.items()}
        
        # Model prediction
        sigmoid = torch.nn.Sigmoid()
        with torch.no_grad():
            print("ü§ñ Running model prediction...")
            outputs = model(**enc)
            logits = outputs.logits
            probs = sigmoid(logits).cpu().numpy()
            print(f"‚úÖ Prediction successful! Output shape: {probs.shape}")
            
    except Exception as e:
        error_msg = f"Prediction error: {str(e)}\n{traceback.format_exc()}"
        print(f"‚ùå {error_msg}")
        return [{"text": text, "probs_sorted": [], "chosen": [], "error": error_msg} for text in texts]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    results = []
    for i, text in enumerate(texts):
        try:
            p = probs[i]
            order = np.argsort(p)[::-1]
            probs_sorted = [(labels[j], float(p[j])) for j in order]

            if top_k and top_k > 0:
                chosen = [labels[j] for j in order[:top_k]]
            else:
                chosen = []
                for name, prob in probs_sorted:
                    if prob >= threshold:
                        chosen.append(name)
                    else:
                        break

            results.append({
                "text": text,
                "probs_sorted": probs_sorted,
                "chosen": chosen,
                "error": None
            })
            
        except Exception as e:
            error_msg = f"Result processing error for text {i}: {str(e)}"
            print(f"‚ùå {error_msg}")
            results.append({
                "text": text,
                "probs_sorted": [],
                "chosen": [],
                "error": error_msg
            })
            
    print(f"‚úÖ Generated {len(results)} results")
    return results

# ‡πÇ‡∏´‡∏•‡∏î model ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
if not st.session_state.model_loaded:
    with st.spinner("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•..."):
        model, tokenizer, labels, error = load_model_and_tokenizer()
        
        if model is not None:
            st.session_state.model = model
            st.session_state.tokenizer = tokenizer
            st.session_state.labels = labels
            st.session_state.model_loaded = True
            st.session_state.error_message = ""
            st.success("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        else:
            st.session_state.error_message = error
            st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ: {error}")

# UI
st.title("üì∞ Thai News Classification")
st.markdown("‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• `wcberta-prachathai67k`")

# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ model
if st.session_state.model_loaded:
    st.success("üü¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
else:
    st.error("üî¥ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    if st.session_state.error_message:
        with st.expander("‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Error"):
            st.text(st.session_state.error_message)

# Debug information
with st.expander("üîß ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Debug"):
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**Device:** {device}")
        st.write(f"**Model Loaded:** {st.session_state.model_loaded}")
    with col2:
        if st.session_state.tokenizer:
            st.write(f"**Vocab Size:** {st.session_state.tokenizer.vocab_size}")
        if st.session_state.labels:
            st.write(f"**Labels:** {len(st.session_state.labels)}")

st.write("‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
input_texts = st.text_area("‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:", height=200, 
                          placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏Å‡∏£‡∏°‡∏≠‡∏∏‡∏ï‡∏∏‡∏Ø ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏ó‡∏¢‡πÄ‡∏à‡∏≠‡∏ù‡∏ô‡∏ñ‡∏•‡πà‡∏°...")

if st.button("üß† ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•"):
    if not st.session_state.model_loaded:
        st.error("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà")
    elif not input_texts.strip():
        st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•")
    else:
        valid_texts = [input_texts.strip()]
        
        with st.spinner("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°..."):
            results = predict_with_probs(
                valid_texts, 
                st.session_state.model,
                st.session_state.tokenizer,
                st.session_state.labels,
                threshold=None, 
                top_k=3
            )
            
            # Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            st.markdown(f"**Debug Results:** `{len(results)}` items")
            for i, result in enumerate(results):
                st.markdown(f"Result {i}: {len(result.get('probs_sorted', []))} probabilities")
            
            if results and any(result.get('probs_sorted') for result in results):
                st.success("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                
                for result in results:
                    if result.get('error'):
                        st.error(f"Error: {result['error']}")
                        continue
                        
                    with st.container(border=True):
                        st.markdown(f"**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:** {result['text']}")
                        
                        if result['chosen']:
                            tags = ' '.join([f"`{cat}`" for cat in result['chosen']])
                            st.markdown(f"**‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:** {tags}")
                        else:
                            st.markdown("**‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:** ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÑ‡∏î‡πâ")

                        with st.expander("‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
                            if result['probs_sorted']:
                                df = pd.DataFrame(result['probs_sorted'], columns=['‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô'])
                                st.dataframe(df)
                            else:
                                st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô")
            else:
                st.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤
                if results:
                    for i, result in enumerate(results):
                        if result.get('error'):
                            st.text(f"Error in result {i}: {result['error']}")

# ‡∏õ‡∏∏‡πà‡∏°‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï model
if st.button("üîÑ ‡∏£‡∏µ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•"):
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.experimental_rerun()