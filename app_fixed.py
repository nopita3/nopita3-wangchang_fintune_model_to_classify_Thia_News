import json, torch, numpy as np
import traceback
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit as st
import pandas as pd

st.set_page_config(
    page_title="Thai News Classification",
    page_icon="üì∞",
    layout="centered"
)

MODEL_DIR = "wcberta-prachathai67k-best"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà save_pretrained ‡πÑ‡∏ß‡πâ
MAX_LEN   = 512
THRESH    = 0.5   



# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î device ‡∏Å‡πà‡∏≠‡∏ô
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initialize session state for model
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.session_state.error_message = ""

def load_model_and_tokenizer():
    """‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏•‡∏∞ tokenizer ‡∏û‡∏£‡πâ‡∏≠‡∏° error handling"""
    try:
        print(f"Loading tokenizer from {MODEL_DIR}...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
        
        print(f"Loading model from {MODEL_DIR}...")
        # ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î model
        model = None
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        try:
            model = AutoModelForSequenceClassification.from_pretrained(
                MODEL_DIR, 
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=False
            )
            model = model.to(device)
            model.eval()
            print("‚úÖ Model loaded successfully with method 1")
            
        except Exception as e1:
            print(f"‚ùå Method 1 failed: {e1}")
            
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ to_empty()
            try:
                model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
                if hasattr(model, 'to_empty'):
                    model = model.to_empty(device=device)
                else:
                    model = model.to(device)
                model.eval()
                print("‚úÖ Model loaded successfully with method 2")
                
            except Exception as e2:
                print(f"‚ùå Method 2 failed: {e2}")
                
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô CPU ‡∏Å‡πà‡∏≠‡∏ô
                try:
                    model = AutoModelForSequenceClassification.from_pretrained(
                        MODEL_DIR,
                        torch_dtype=torch.float32
                    )
                    model = model.to(device)
                    model.eval()
                    print("‚úÖ Model loaded successfully with method 3")
                except Exception as e3:
                    print(f"‚ùå All methods failed: {e3}")
                    raise e3
        
        # ‡πÇ‡∏´‡∏•‡∏î labels
        with open(f"{MODEL_DIR}/label_names.json", encoding="utf-8") as f:
            labels = json.load(f)
        
        print(f"‚úÖ All components loaded successfully!")
        print(f"üìä Device: {device}")
        print(f"üìö Vocab size: {tokenizer.vocab_size}")
        print(f"üè∑Ô∏è Number of labels: {len(labels)}")
        
        return model, tokenizer, labels
        
    except Exception as e:
        error_msg = f"Failed to load model: {str(e)}\n{traceback.format_exc()}"
        print(f"‚ùå {error_msg}")
        return None, None, None, error_msg

# ‡πÇ‡∏´‡∏•‡∏î tokenizer


def predict_with_probs(texts: list[str], threshold: float = THRESH, top_k: int = None):
    """
    ‡∏Ñ‡∏∑‡∏ô:
      - probs_sorted: ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ + prob ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
      - chosen: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå" (threshold ‡∏´‡∏£‡∏∑‡∏≠ top_k)
    """
    model, tokenizer, labels = load_model_and_tokenizer() 
    
    if model is None or tokenizer is None:
        error_msg = "Model ‡∏´‡∏£‡∏∑‡∏≠ tokenizer ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î"
        print(f"‚ùå {error_msg}")
        return [{"text": text, "probs_sorted": [], "chosen": [], "error": error_msg} for text in texts]

    try:
        # Tokenize with safe parameters
        enc = tokenizer(texts, return_tensors="pt", 
                  truncation=True, 
                  max_length=MAX_LEN, 
                  padding=True,
                  return_token_type_ids=False)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç token IDs ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
        vocab_size = tokenizer.vocab_size
        if torch.any(enc['input_ids'] >= vocab_size):
            print(f"WARNING: Found token IDs >= vocab_size ({vocab_size})")
            enc['input_ids'] = torch.clamp(enc['input_ids'], 0, vocab_size - 1)
        
        if torch.any(enc['input_ids'] < 0):
            print("WARNING: Found negative token IDs")
            enc['input_ids'] = torch.clamp(enc['input_ids'], 0, vocab_size - 1)
        
        
        enc = {k: v.to(device) for k, v in enc.items()}

        sigmoid = torch.nn.Sigmoid()
        with torch.no_grad():
            logits = model(**enc).logits
            probs = sigmoid(logits).cpu().numpy()  # (B, C)
            
    except Exception as e:
        print(f"Error in tokenization or model prediction: {e}")
        # Return empty results if error occurs
        return [{"text": text, "probs_sorted": [], "chosen": []} for text in texts]

    results = []
    for i, text in enumerate(texts):
        p = probs[i]                                 # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™
        order = np.argsort(p)[::-1]                  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡∏Å‚Üí‡∏ô‡πâ‡∏≠‡∏¢
        probs_sorted = [(labels[j], float(p[j])) for j in order]

        if top_k and top_k > 0:
            chosen = [labels[j] for j in order[:top_k]]
        else:
            chosen = []
            for name, prob in probs_sorted:
                if prob >= threshold:
                    chosen.append(name)
                else:
                    break

        results.append({
            "text": text,
            "probs_sorted": probs_sorted,  # ‡∏î‡∏π‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ
            "chosen": chosen               # ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™
        })
    return results

# ‡πÇ‡∏´‡∏•‡∏î model ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
if not st.session_state.model_loaded:
    with st.spinner("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•..."):
        model, tokenizer, labels = load_model_and_tokenizer()
        
        if model is not None:
            st.session_state.model = model
            st.session_state.tokenizer = tokenizer
            st.session_state.labels = labels
            st.session_state.model_loaded = True
            st.session_state.error_message = ""
            st.success("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        else:
            st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ: ")

st.title("üì∞ Thai News Classification")
st.markdown("‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• `wcberta-prachathai67k`")

# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ model
if st.session_state.model_loaded:
    st.success("üü¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
else:
    st.error("üî¥ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    if st.session_state.error_message:
        with st.expander("‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Error"):
            st.text(st.session_state.error_message)


st.write("‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)")
input_texts = st.text_area("‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 512 ‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î):", height=200, placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏Å‡∏£‡∏°‡∏≠‡∏∏‡∏ï‡∏∏‡∏Ø ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏ó‡∏¢‡πÄ‡∏à‡∏≠‡∏ù‡∏ô‡∏ñ‡∏•‡πà‡∏°...")

if st.button("üß† ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•"):
    if not input_texts.strip():
        st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•")
    else:
        valid_texts = [input_texts.strip()]
        for text in valid_texts:
            if len(text.split()) > 512:
                st.error(f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà  ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 512 ‡∏Ñ‡∏≥: '{text[:50]}...'")
            

        if valid_texts:
            with st.spinner("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°..."):
                try:
                    results = predict_with_probs(valid_texts, threshold=None, top_k=3)
                    st.markdown(f"‡∏ú‡∏• {results}")
                    
                    if results :
                        st.success("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                        
                        st.subheader("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:")
                        for result in results:
                            with st.container(border=True):
                                st.markdown(f"**‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:** {result['text']}")
                                
                                tags = ' '.join([f"`{cat}`" for cat in result['chosen']])
                                st.markdown(f"**‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢:** {tags}")

                                with st.expander("‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
                                    df = pd.DataFrame(result['probs_sorted'], columns=['‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô'])
                                    st.dataframe(df, width='stretch')
                    else:
                        st.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
                        
                except Exception as e:
                    st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
                    st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
                    
# ‡∏õ‡∏∏‡πà‡∏°‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï model
if st.button("üîÑ ‡∏£‡∏µ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•"):
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.experimental_rerun()