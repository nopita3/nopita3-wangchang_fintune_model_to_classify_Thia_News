import json, torch, numpy as np
import traceback
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit as st
import pandas as pd

st.set_page_config(
    page_title="Thai News Classification",
    page_icon="ðŸ“°",
    layout="centered"
)

MODEL_DIR = "wcberta-prachathai67k-best"
MAX_LEN = 512
THRESH = 0.5   

# à¸à¸³à¸«à¸™à¸” device à¸à¹ˆà¸­à¸™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initialize session state for model
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.session_state.error_message = ""

def load_model_and_tokenizer():
    """à¹‚à¸«à¸¥à¸” model à¹à¸¥à¸° tokenizer à¸žà¸£à¹‰à¸­à¸¡ error handling"""
    try:
        print(f"Loading tokenizer from {MODEL_DIR}...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
        
        print(f"Loading model from {MODEL_DIR}...")
        model = None
        
        # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: à¹‚à¸«à¸¥à¸”à¹à¸šà¸šà¸›à¸à¸•à¸´
        try:
            model = AutoModelForSequenceClassification.from_pretrained(
                MODEL_DIR, 
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=False
            )
            model = model.to(device)
            model.eval()
            print("âœ… Model loaded successfully with method 1")
            
        except Exception as e1:
            print(f"âŒ Method 1 failed: {e1}")
            
            # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: à¹ƒà¸Šà¹‰ to_empty()
            try:
                model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
                if hasattr(model, 'to_empty'):
                    model = model.to_empty(device=device)
                else:
                    model = model.to(device)
                model.eval()
                print("âœ… Model loaded successfully with method 2")
                
            except Exception as e2:
                print(f"âŒ Method 2 failed: {e2}")
                
                # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 3: à¹‚à¸«à¸¥à¸”à¹ƒà¸™ CPU à¸à¹ˆà¸­à¸™
                try:
                    model = AutoModelForSequenceClassification.from_pretrained(
                        MODEL_DIR,
                        torch_dtype=torch.float32
                    )
                    model = model.to(device)
                    model.eval()
                    print("âœ… Model loaded successfully with method 3")
                except Exception as e3:
                    print(f"âŒ All methods failed: {e3}")
                    raise e3
        
        # à¹‚à¸«à¸¥à¸” labels
        with open(f"{MODEL_DIR}/label_names.json", encoding="utf-8") as f:
            labels = json.load(f)
        
        print(f"âœ… All components loaded successfully!")
        print(f"ðŸ“Š Device: {device}")
        print(f"ðŸ“š Vocab size: {tokenizer.vocab_size}")
        print(f"ðŸ·ï¸ Number of labels: {len(labels)}")
        
        return model, tokenizer, labels, ""  # success case
        
    except Exception as e:
        error_msg = f"Failed to load model: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        return None, None, None, error_msg  # error case

def predict_with_probs(texts: list[str], model, tokenizer, labels, threshold: float = THRESH, top_k: int = None):
    """à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ model à¸—à¸µà¹ˆà¹‚à¸«à¸¥à¸”à¹à¸¥à¹‰à¸§"""
    if model is None or tokenizer is None:
        error_msg = "Model à¸«à¸£à¸·à¸­ tokenizer à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸–à¸¹à¸à¹‚à¸«à¸¥à¸”"
        print(f"âŒ {error_msg}")
        return [{"text": text, "probs_sorted": [], "chosen": [], "error": error_msg} for text in texts]

    try:
        print(f"ðŸ”¤ Processing {len(texts)} texts")
        
        # Tokenize with safe parameters
        enc = tokenizer(texts, return_tensors="pt", 
                  truncation=True, 
                  max_length=MAX_LEN, 
                  padding=True,
                  return_token_type_ids=False)
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸¥à¸°à¹à¸à¹‰à¹„à¸‚ token IDs à¸—à¸µà¹ˆà¹€à¸à¸´à¸™à¸‚à¸­à¸šà¹€à¸‚à¸•
        vocab_size = tokenizer.vocab_size
        if torch.any(enc['input_ids'] >= vocab_size):
            print(f"âš ï¸ WARNING: Found token IDs >= vocab_size ({vocab_size})")
            enc['input_ids'] = torch.clamp(enc['input_ids'], 0, vocab_size - 1)
        
        if torch.any(enc['input_ids'] < 0):
            print("âš ï¸ WARNING: Found negative token IDs")
            enc['input_ids'] = torch.clamp(enc['input_ids'], 0, vocab_size - 1)
        
        enc = {k: v.to(device) for k, v in enc.items()}

        sigmoid = torch.nn.Sigmoid()
        with torch.no_grad():
            print("ðŸ¤– Running model prediction...")
            logits = model(**enc).logits
            probs = sigmoid(logits).cpu().numpy()
            print(f"âœ… Prediction successful! Shape: {probs.shape}")
            
    except Exception as e:
        error_msg = f"Error in tokenization or model prediction: {e}"
        print(f"âŒ {error_msg}")
        return [{"text": text, "probs_sorted": [], "chosen": [], "error": error_msg} for text in texts]

    results = []
    for i, text in enumerate(texts):
        try:
            p = probs[i]
            order = np.argsort(p)[::-1]
            probs_sorted = [(labels[j], float(p[j])) for j in order]

            if top_k and top_k > 0:
                chosen = [labels[j] for j in order[:top_k]]
            else:
                chosen = []
                for name, prob in probs_sorted:
                    if prob >= threshold:
                        chosen.append(name)
                    else:
                        break

            results.append({
                "text": text,
                "probs_sorted": probs_sorted,
                "chosen": chosen,
                "error": None
            })
        except Exception as e:
            results.append({
                "text": text,
                "probs_sorted": [],
                "chosen": [],
                "error": str(e)
            })
    return results

# à¹‚à¸«à¸¥à¸” model à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™
if not st.session_state.model_loaded:
    with st.spinner("ðŸš€ à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥..."):
        model, tokenizer, labels, error = load_model_and_tokenizer()
        
        if model is not None:
            st.session_state.model = model
            st.session_state.tokenizer = tokenizer
            st.session_state.labels = labels
            st.session_state.model_loaded = True
            st.session_state.error_message = ""
        else:
            st.session_state.error_message = error

st.title("ðŸ“° Thai News Classification")
st.markdown("à¹à¸­à¸›à¸žà¸¥à¸´à¹€à¸„à¸Šà¸±à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸³à¹à¸™à¸à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹ˆà¸²à¸§à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ `wcberta-prachathai67k`")

# à¹à¸ªà¸”à¸‡à¸ªà¸–à¸²à¸™à¸° model
if st.session_state.model_loaded:
    st.success("ðŸŸ¢ à¹‚à¸¡à¹€à¸”à¸¥à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
else:
    st.error("ðŸ”´ à¹‚à¸¡à¹€à¸”à¸¥à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
    if st.session_state.error_message:
        with st.expander("à¸”à¸¹à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” Error"):
            st.text(st.session_state.error_message)

st.write("à¸›à¹‰à¸­à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢:")
input_texts = st.text_area("à¹ƒà¸ªà¹ˆà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸™à¸µà¹ˆ:", height=200, 
                          placeholder="à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: à¸à¸£à¸¡à¸­à¸¸à¸•à¸¸à¸¯ à¹€à¸•à¸·à¸­à¸™à¸§à¸±à¸™à¸™à¸µà¹‰à¸—à¸±à¹ˆà¸§à¹„à¸—à¸¢à¹€à¸ˆà¸­à¸à¸™à¸–à¸¥à¹ˆà¸¡...")

if st.button("ðŸ§  à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥"):
    if not st.session_state.model_loaded:
        st.error("à¹‚à¸¡à¹€à¸”à¸¥à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸–à¸¹à¸à¹‚à¸«à¸¥à¸” à¸à¸£à¸¸à¸“à¸²à¸£à¸µà¹€à¸Ÿà¸£à¸Šà¸«à¸™à¹‰à¸²à¹€à¸žà¸·à¹ˆà¸­à¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆ")
    elif not input_texts.strip():
        st.warning("à¸à¸£à¸¸à¸“à¸²à¸›à¹‰à¸­à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸žà¸·à¹ˆà¸­à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥")
    else:
        valid_texts = [input_texts.strip()]
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸¢à¸²à¸§
        text_too_long = False
        for text in valid_texts:
            if len(text.split()) > 512:
                st.error(f"à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¡à¸µà¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹€à¸à¸´à¸™ 512 à¸„à¸³: '{text[:50]}...'")
                text_too_long = True

        if valid_texts and not text_too_long:
            with st.spinner("ðŸ¤– à¸à¸³à¸¥à¸±à¸‡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡..."):
                try:
                    results = predict_with_probs(
                        valid_texts, 
                        st.session_state.model,
                        st.session_state.tokenizer,
                        st.session_state.labels,
                        threshold=None, 
                        top_k=3
                    )
                    
                    # Debug output
                    st.text(f"Debug: à¹„à¸”à¹‰à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ {len(results)} à¸£à¸²à¸¢à¸à¸²à¸£")
                    
                    if results and any(result.get('probs_sorted') for result in results):
                        st.success("à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ!")
                        
                        for result in results:
                            if result.get('error'):
                                st.error(f"Error: {result['error']}")
                                continue
                                
                            with st.container(border=True):
                                st.markdown(f"**à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡:** {result['text']}")
                                
                                if result['chosen']:
                                    tags = ' '.join([f"`{cat}`" for cat in result['chosen']])
                                    st.markdown(f"**à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢:** {tags}")
                                else:
                                    st.markdown("**à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢:** à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ˆà¸³à¹à¸™à¸à¹„à¸”à¹‰")

                                with st.expander("à¸”à¸¹à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”"):
                                    if result['probs_sorted']:
                                        df = pd.DataFrame(result['probs_sorted'], columns=['à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ', 'à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™'])
                                        st.dataframe(df)
                                    else:
                                        st.write("à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™")
                    else:
                        st.error("à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡")
                        
                except Exception as e:
                    st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”: {str(e)}")
                    st.info("à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹‚à¸¡à¹€à¸”à¸¥à¸–à¸¹à¸à¹‚à¸«à¸¥à¸”à¸­à¸¢à¹ˆà¸²à¸‡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")

# à¸›à¸¸à¹ˆà¸¡à¸£à¸µà¹€à¸‹à¹‡à¸• model
if st.button("ðŸ”„ à¸£à¸µà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥"):
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    
 
    st.rerun()  # Streamlit >= 1.27.0
