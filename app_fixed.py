import json, torch, numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit as st
import pandas as pd

st.set_page_config(
    page_title="Thai News Classification",
    page_icon="ðŸ“°",
    layout="centered"
)

MODEL_DIR = "wcberta-prachathai67k-best"  # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸µà¹ˆ save_pretrained à¹„à¸§à¹‰
MAX_LEN   = 512
THRESH    = 0.5   

# à¸à¸³à¸«à¸™à¸” device à¸à¹ˆà¸­à¸™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.session_state.error_message = ""

def load_model_and_tokenizer():
    # à¹‚à¸«à¸¥à¸” tokenizer
    tok = AutoTokenizer.from_pretrained(MODEL_DIR)

    # à¹‚à¸«à¸¥à¸” model à¹à¸¥à¸°à¸ˆà¸±à¸”à¸à¸²à¸£ meta tensor
    try:
        # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: à¹‚à¸«à¸¥à¸” model à¹‚à¸”à¸¢à¸•à¸£à¸‡à¹„à¸›à¸¢à¸±à¸‡ device à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
        model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_DIR, 
            dtype=torch.float32,
            device_map=None,  # à¸›à¸´à¸” automatic device mapping
            low_cpu_mem_usage=False
        )
        model = model.to(device)
        model.eval()
        print("Model loaded successfully with direct device mapping")
        
    except Exception as e:
        print(f"Direct loading failed: {e}")
        try:
            # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: à¹ƒà¸Šà¹‰ to_empty() à¸ªà¸³à¸«à¸£à¸±à¸š meta tensor
            model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
            if hasattr(model, 'to_empty'):
                model = model.to_empty(device=device)
            else:
                model = model.to(device)
            model.eval()
            print("Model loaded successfully with to_empty method")
            
        except Exception as e2:
            print(f"to_empty method failed: {e2}")
            # à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 3: à¹‚à¸«à¸¥à¸”à¹à¸šà¸š CPU à¹à¸¥à¹‰à¸§ move à¸—à¸µà¸¥à¸°à¸Šà¸´à¹‰à¸™
            model = AutoModelForSequenceClassification.from_pretrained(
                MODEL_DIR,
                dtype=torch.float32,
                device_map="cpu"
            )
            model = model.to(device)
            model.eval()
            print("Model loaded successfully with CPU first method")


    with open(f"{MODEL_DIR}/label_names.json", encoding="utf-8") as f:
        LABELS = json.load(f)

    return model, tok, LABELS

model, tok, LABELS = load_model_and_tokenizer()

def predict_with_probs(texts: list[str], threshold: float = THRESH, top_k: int = None):
    """
    à¸„à¸·à¸™:
      - probs_sorted: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¸„à¸¥à¸²à¸ª + prob à¹€à¸£à¸µà¸¢à¸‡à¸ˆà¸²à¸à¸¡à¸²à¸à¹„à¸›à¸™à¹‰à¸­à¸¢
      - chosen: à¸£à¸²à¸¢à¸à¸²à¸£à¸„à¸¥à¸²à¸ªà¸—à¸µà¹ˆ "à¸œà¹ˆà¸²à¸™à¹€à¸à¸“à¸‘à¹Œ" (threshold à¸«à¸£à¸·à¸­ top_k)
    """
    try:
        # Tokenize with safe parameters
        enc = tok(texts, return_tensors="pt", 
                  truncation=True, 
                  max_length=MAX_LEN, 
                  padding=True,
                  return_token_type_ids=False)
        enc = {k: v.to(device) for k, v in enc.items()}

        sigmoid = torch.nn.Sigmoid()
        with torch.no_grad():
            logits = model(**enc).logits
            probs = sigmoid(logits).cpu().numpy()  # (B, C)
            
    except Exception as e:
        print(f"Error in tokenization or model prediction: {e}")
        # Return empty results if error occurs
        return [{"text": text, "probs_sorted": [], "chosen": []} for text in texts]

    results = []
    for i, text in enumerate(texts):
        p = probs[i]                                 # à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸•à¹ˆà¸­à¸„à¸¥à¸²à¸ª
        order = np.argsort(p)[::-1]                  # à¹€à¸£à¸µà¸¢à¸‡à¸¡à¸²à¸â†’à¸™à¹‰à¸­à¸¢
        probs_sorted = [(LABELS[j], float(p[j])) for j in order]

        if top_k and top_k > 0:
            chosen = [LABELS[j] for j in order[:top_k]]
        else:
            chosen = []
            for name, prob in probs_sorted:
                if prob >= threshold:
                    chosen.append(name)
                else:
                    break

        results.append({
            "text": text,
            "probs_sorted": probs_sorted,  # à¸”à¸¹à¸­à¸±à¸™à¸”à¸±à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹„à¸”à¹‰
            "chosen": chosen               # à¸„à¸³à¸•à¸­à¸šà¸«à¸¥à¸²à¸¢à¸„à¸¥à¸²à¸ª
        })
    return results

# à¹‚à¸«à¸¥à¸” model à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™
if not st.session_state.model_loaded:
    with st.spinner("ðŸš€ à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥..."):
        model, tokenizer, labels, error = load_model_and_tokenizer()
        
        if model is not None:
            st.session_state.model = model
            st.session_state.tokenizer = tokenizer
            st.session_state.labels = labels
            st.session_state.model_loaded = True
            st.session_state.error_message = ""
        else:
            st.session_state.error_message = error

st.title("ðŸ“° Thai News Classification")
st.markdown("à¹à¸­à¸›à¸žà¸¥à¸´à¹€à¸„à¸Šà¸±à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸³à¹à¸™à¸à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹ˆà¸²à¸§à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ `wcberta-prachathai67k`")

if st.session_state.model_loaded:
    st.success("ðŸŸ¢ à¹‚à¸¡à¹€à¸”à¸¥à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
else:
    st.error("ðŸ”´ à¹‚à¸¡à¹€à¸”à¸¥à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
    if st.session_state.error_message:
        with st.expander("à¸”à¸¹à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” Error"):
            st.text(st.session_state.error_message)

st.write("à¸›à¹‰à¸­à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢ (à¸«à¸™à¸¶à¹ˆà¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸•à¹ˆà¸­à¸«à¸™à¸¶à¹ˆà¸‡à¸šà¸£à¸£à¸—à¸±à¸”)")
input_texts = st.text_area("à¹ƒà¸ªà¹ˆà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸™à¸µà¹ˆ (à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 512 à¸„à¸³à¸•à¹ˆà¸­à¸šà¸£à¸£à¸—à¸±à¸”):", height=200, placeholder="à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: à¸à¸£à¸¡à¸­à¸¸à¸•à¸¸à¸¯ à¹€à¸•à¸·à¸­à¸™à¸§à¸±à¸™à¸™à¸µà¹‰à¸—à¸±à¹ˆà¸§à¹„à¸—à¸¢à¹€à¸ˆà¸­à¸à¸™à¸–à¸¥à¹ˆà¸¡...")

if st.button("ðŸ§  à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥"):
    if not input_texts.strip():
        st.warning("à¸à¸£à¸¸à¸“à¸²à¸›à¹‰à¸­à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸žà¸·à¹ˆà¸­à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥")
    else:
        valid_texts = [input_texts.strip()]
        for text in valid_texts:
            if len(text.split()) > 512:
                st.error(f"à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆ  à¸¡à¸µà¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹€à¸à¸´à¸™ 512 à¸„à¸³: '{text[:50]}...'")
            

        if valid_texts:
            with st.spinner("ðŸ¤– à¸à¸³à¸¥à¸±à¸‡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡..."):
                try:
                    results = predict_with_probs(valid_texts, threshold=None, top_k=3)
                    if results[0]["probs_sorted"] is None: 
                        st.error("à¹„à¸¡à¹ˆà¸žà¸šà¸œà¸¥à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢")
                    
                    if results :
                        st.success("à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ!")
                        
                        st.subheader("à¸œà¸¥à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢:")
                        for result in results:
                            with st.container(border=True):
                                st.markdown(f"**à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡:** {result['text']}")
                                
                                tags = ' '.join([f"`{cat}`" for cat in result['chosen']])
                                st.markdown(f"**à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢:** {tags}")

                                with st.expander("à¸”à¸¹à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”"):
                                    df = pd.DataFrame(result['probs_sorted'], columns=['à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ', 'à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™'])
                                    st.dataframe(df, width='stretch')
                    else:
                        st.error("à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡")
                        
                except Exception as e:
                    st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”: {str(e)}")
                    st.info("à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹‚à¸¡à¹€à¸”à¸¥à¸–à¸¹à¸à¹‚à¸«à¸¥à¸”à¸­à¸¢à¹ˆà¸²à¸‡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")

if st.button("ðŸ”„ à¸£à¸µà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥"):
    st.session_state.model_loaded = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.labels = None
    st.rerun()  # Streamlit >= 1.27.0